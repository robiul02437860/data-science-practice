{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749b69d-a274-4a75-8717-3f9f9cb49164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44dcb30-8485-406f-9917-40b6abafc41e",
   "metadata": {},
   "source": [
    "# Dog Movement Analysis\n",
    "Obtained from: https://www.sciencedirect.com/science/article/pii/S2352340922000348"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b682cf4-8a63-4f7a-8589-4318e56a20f1",
   "metadata": {},
   "source": [
    "## Load, Clean, and Split Dog Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3c084-65a2-45ea-bdf1-f91090c96c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_raw = pd.read_csv(\"DogMoveData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c27f8-6277-42a1-9147-173f1714ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_raw['second'] = dog_raw['t_sec'].round()\n",
    "dog_raw['minute'] = dog_raw['t_sec'].round() // 60\n",
    "dog_clean = dog_raw.loc[:, ['DogID', 'TestNum', 'minute', 'Task', \n",
    "                       'ABack_x', 'ABack_y', 'ABack_z',\n",
    "                       'ANeck_x', 'ANeck_y', 'ANeck_z',\n",
    "                       'GBack_x', 'GBack_y', 'GBack_z',\n",
    "                       'GNeck_x', 'GNeck_y', 'GNeck_z'\n",
    "                      ]]\n",
    "dog_clean = dog_clean.groupby(['DogID', 'TestNum', 'minute', 'Task']).agg(['mean', 'std', 'min', 'max'])\n",
    "dog_clean.columns = ['_'.join(col).strip() for col in dog_clean.columns.values]\n",
    "dog_clean = dog_clean.reset_index()\n",
    "dog_clean = dog_clean.loc[dog_clean.Task != '<undefined>', :].dropna()\n",
    "dog_clean.Task = dog_clean.Task.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21dae4-ba66-45c0-b42a-7f9a93192dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dog_clean.drop(columns=['Task', 'DogID', 'TestNum', 'minute']), dog_clean.Task, test_size=0.2, random_state=42)\n",
    "\n",
    "x_vars = X_train.columns\n",
    "\n",
    "standardize = StandardScaler()\n",
    "X_train = standardize.fit_transform(X_train)\n",
    "X_test = standardize.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=x_vars)\n",
    "X_test = pd.DataFrame(X_test, columns=x_vars)\n",
    "\n",
    "train = X_train.copy() \n",
    "train['Task'] = y_train\n",
    "\n",
    "print(\"training data shape\", train.shape)\n",
    "print(\"test samples\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de30fd-1951-4077-b4f8-6df4da826998",
   "metadata": {},
   "source": [
    "## Dog Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d8af0-1769-452d-9ad8-a3c1298cb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.value_counts('Task').plot.barh()\n",
    "plt.title(\"Distribution of Tasks\")\n",
    "plt.xlabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422865d-ab7d-4982-9658-f7f77da91d40",
   "metadata": {},
   "source": [
    "The fairly even distribution of classes suggest that no class imbalance techniques need to be employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc9512-0a66-42da-8eb7-226bbbe6207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_long = train.melt(id_vars='Task')\n",
    "train_long[['bodypart', 'direction', 'metric']] = train_long['variable'].str.split('_', expand=True)\n",
    "train_long['device'] = train_long['bodypart'].str[0]\n",
    "train_long['bodypart'] = train_long['bodypart'].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc210c36-7400-40a8-a82d-182bebdca21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_mean = train_long.loc[(train_long.device == \"A\") & (train_long.metric == 'mean'), :]\n",
    "accel_grid = sns.FacetGrid(accel_mean, col=\"bodypart\", row='direction', hue=\"Task\", sharex=False, sharey=False)\n",
    "accel_grid.map_dataframe(sns.kdeplot, x=\"value\")\n",
    "accel_grid.add_legend()\n",
    "accel_grid.set(xlim=(-2,2))\n",
    "accel_grid.fig.suptitle(\"Mean Acceleration\", y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b86df-4a20-44b2-987d-890b282802a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_std = train_long.loc[(train_long.device == \"A\") & (train_long.metric == 'std'), :]\n",
    "accel_grid = sns.FacetGrid(accel_std, col=\"bodypart\", row='direction', hue=\"Task\", sharex=False, sharey=False)\n",
    "accel_grid.map_dataframe(sns.kdeplot, x=\"value\")\n",
    "accel_grid.add_legend()\n",
    "accel_grid.set(xlim=(-2,2))\n",
    "accel_grid.fig.suptitle(\"Standard Deviation of Acceleration\", y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f78738-e98a-47f1-9e8d-9d3a5e20fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_max = train_long.loc[(train_long.device == \"A\") & (train_long.metric == 'max'), :]\n",
    "accel_grid = sns.FacetGrid(accel_max, col=\"bodypart\", row='direction', hue=\"Task\", sharex=False, sharey=False)\n",
    "accel_grid.map_dataframe(sns.kdeplot, x=\"value\")\n",
    "accel_grid.add_legend()\n",
    "accel_grid.set(xlim=(-2,2))\n",
    "accel_grid.fig.suptitle(\"Max Acceleration\", y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb798f-35d1-4e3b-80d1-8a3eb132a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "gyro_mean = train_long.loc[(train_long.device == \"G\") & (train_long.metric == 'mean') & (train_long.Task != \"Task lie down\") & (train_long.Task != \"Task sit\") & (train_long.Task != \"Task stand\"), :]\n",
    "gyro_grid = sns.FacetGrid(gyro_mean, col=\"bodypart\", row='direction', hue=\"Task\", sharex=False, sharey=False)\n",
    "gyro_grid.map_dataframe(sns.kdeplot, x=\"value\")\n",
    "gyro_grid.add_legend()\n",
    "gyro_grid.set(xlim=(-20,20))\n",
    "gyro_grid.fig.suptitle(\"Mean Gyroscope\", y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa259d-d711-4ca1-9705-82fcabf80da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gyro_std = train_long.loc[(train_long.device == \"G\") & (train_long.metric == 'std') & (train_long.Task != \"Task lie down\") & (train_long.Task != \"Task sit\") & (train_long.Task != \"Task stand\"), :]\n",
    "gyro_grid = sns.FacetGrid(gyro_std, col=\"bodypart\", row='direction', hue=\"Task\", sharex=False, sharey=False)\n",
    "gyro_grid.map_dataframe(sns.kdeplot, x=\"value\")\n",
    "gyro_grid.add_legend()\n",
    "gyro_grid.set(xlim=(-20,20))\n",
    "gyro_grid.fig.suptitle(\"Standard Deviation Gyroscope\", y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f1d9e-0d0b-4638-8ada-7ee5a679da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "gyro_max = train_long.loc[(train_long.device == \"G\") & (train_long.metric == 'max') & (train_long.Task != \"Task lie down\") & (train_long.Task != \"Task sit\") & (train_long.Task != \"Task stand\"), :]\n",
    "gyro_grid = sns.FacetGrid(gyro_max, col=\"bodypart\", row='direction', hue=\"Task\", sharex=False, sharey=False)\n",
    "gyro_grid.map_dataframe(sns.kdeplot, x=\"value\")\n",
    "gyro_grid.add_legend()\n",
    "gyro_grid.set(xlim=(-20,20))\n",
    "gyro_grid.fig.suptitle(\"Max Gyroscope\", y = 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070de14-0133-4e87-a926-27d48481c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train, x='ABack_y_mean', y = 'ABack_z_mean', hue='Task')\n",
    "plt.title(\"True Data Points Between Mean Z\\nAcceleration vs Mean Y Acceleration on Back\")\n",
    "plt.xlabel(\"Mean Y Acceleration\")\n",
    "plt.ylabel(\"Mean Z Acceleration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665945e-1fbc-453b-b7bf-63ba7838a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train, x='ABack_x_mean', y = 'ABack_y_mean', hue='Task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e59428-6785-4c07-b15c-64ae4bcccb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train, x='ANeck_x_mean', y = 'ANeck_y_mean', hue='Task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7f7a2-29b6-4c34-bd45-31c34e292807",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train, x='ANeck_y_mean', y = 'ANeck_z_mean', hue='Task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5706c22-0ba0-4ec8-94c0-131ddd6c0532",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train, x='ABack_y_mean', y = 'ANeck_z_mean', hue='Task')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93bcb9c-bacc-4211-a933-f6828ca31871",
   "metadata": {},
   "source": [
    "Most plots are hard to distinguish separations among the data, however some variables and plots show more than others. In general the y and z measurements show more separation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea021cd-45f9-4339-81d8-f8596fa82631",
   "metadata": {},
   "source": [
    "## SVC (One vs One) on Dog Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56c5c8-451d-4579-8806-01a3064b7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_svc_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel' : ['rbf', 'poly']\n",
    "}\n",
    "dog_svc_cv = GridSearchCV(svm.SVC(), dog_svc_grid, cv=10, n_jobs = -1, verbose=True)\n",
    "dog_svc_cv.fit(X_train, y_train)\n",
    "print(\"Test Accuracy:\", dog_svc_cv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8a674-9e26-4c81-af2d-13e8e81789c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dog_svc_cv.cv_results_).loc[:, ['param_C', 'param_kernel', 'mean_test_score', 'std_test_score', 'mean_fit_time', 'std_fit_time']].sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698cd99-e8c8-4570-a6ab-2919ddb092cc",
   "metadata": {},
   "source": [
    "## Linear SVC (One vs Rest) on Dog Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aba55b-5eee-4800-ad97-5747db9d7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_linear_svc_grid = {\n",
    "    'C' : [0.1, 1, 10, 100]\n",
    "}\n",
    "dog_linearsvc_cv = GridSearchCV(svm.LinearSVC(), dog_linear_svc_grid, n_jobs=-1, verbose=True)\n",
    "dog_linearsvc_cv.fit(X_train, y_train)\n",
    "print(\"Test Accuracy:\", dog_linearsvc_cv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c7a38-3787-4f29-b758-0bcc453d3683",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dog_linearsvc_cv.cv_results_).loc[:, ['param_C', 'mean_test_score', 'std_test_score', 'mean_fit_time', 'std_fit_time']].sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b7aa7-c810-4f3e-b491-3fa00e186e3e",
   "metadata": {},
   "source": [
    "## Logistic Regression (One vs Rest) on Dog Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5340ea-2e16-4548-91cb-e388bdb9c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_log_grid = {\n",
    "    'C' : [0.01, 0.1, 1, 10]\n",
    "}\n",
    "dog_log_cv = GridSearchCV(LogisticRegression(max_iter=1000), dog_log_grid, cv = 10, n_jobs=-1, verbose=True)\n",
    "dog_log_cv.fit(X_train, y_train)\n",
    "print(\"Test Accuracy:\", dog_log_cv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a526ee-e471-48b5-b8c6-f2848a7709ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dog_log_cv.cv_results_).loc[:, ['param_C', 'mean_test_score', 'std_test_score', 'mean_fit_time', 'std_fit_time']].sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c373e25-8037-40ed-984d-1443de8c2ab0",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Metrics on Dog Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f74137-07af-4789-b700-5697ebf4a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dog_log_cv.predict(X_test)\n",
    "y_labels = ['Task treat-search', 'Task play', 'Task stand', 'Task trot', 'Task sit', 'Task lie down', 'Task walk']\n",
    "p, r, f, s = precision_recall_fscore_support(y_test, y_pred, labels=y_labels)\n",
    "results = pd.DataFrame({'Task':y_labels, 'Precision':p, 'Recall':r, \"F1\":f, \"Support\":s})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0379df-1477-430a-984e-31a67bc630dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=y_test.cat.categories, yticklabels=y_test.cat.categories)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.title(\"Confusion Matrix Of Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d3dc8-685c-41f8-a618-f80ea55044a0",
   "metadata": {},
   "source": [
    "All three models show greater accuracy on the test set than from the 10-fold cross validation. The SVC (OVO) shows the greates accuracy but also the greatest variablity in model performance. The Linear SVC (OVR) shows the lowest accuracy, but also the least amount of variance, suggest high bias and low variance. \n",
    "\n",
    "The RBF kernels outperform almost all of the polynomial kernels. Only degree 3 polynomials were considered however. \n",
    "\n",
    "The Logistic Regression model took the longest to run on average, about twice as long as the SVC (OVO) model. However, with larger sample sizes (such as on the raw time series data itself), the SVC (OVO) model was unfeasable to even run. The complexity of that model grows much faster than the others due to the OVO structure.\n",
    "\n",
    "The confusion matrix, precision, and recall, of class show good results. Most commonly confused activites are less active activites such as among lying down, sitting, or standing when little movement is recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3079f2-3abe-4282-a337-25186d1636f1",
   "metadata": {},
   "source": [
    "## Plot Decision Boundaries on Dog Data Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba7f70-7ddb-49d1-9ebc-3211898892e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = pd.DataFrame(np.random.uniform(-10, 10, (10000, X_train.shape[1])))\n",
    "grid_predictions.columns = X_train.columns\n",
    "grid_predictions['Task'] = dog_linearsvc_cv.predict(grid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5e4ea-026a-484d-a539-41f107a9ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=grid_predictions, x='ABack_y_mean', y = 'ABack_z_mean', hue='Task')\n",
    "plt.title(\"Predicted Points Between Mean Z\\nAcceleration vs. Mean Y Acceleration On Back\")\n",
    "plt.xlabel(\"Mean Y Acceleration\")\n",
    "plt.ylabel(\"Mean Z Acceleration\")\n",
    "# plt.xlim(-2.5, 3.1)\n",
    "plt.ylim(-10.5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23866fd3-1b59-4d14-9b95-493cc018bbc6",
   "metadata": {},
   "source": [
    "This plot shows some decision boundaries or clustering of classes. Sitting (green) is certainly more on the left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a70ebd-ec85-4a3e-8c25-29cbd66d7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=grid_predictions, x='ABack_x_mean', y = 'ABack_y_mean', hue='Task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbde955-5c27-4745-93c5-762c1485c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=grid_predictions, x='ANeck_x_mean', y = 'ANeck_y_mean', hue='Task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8a18f-e1bf-4d2f-9914-34f235bb41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=grid_predictions, x='ANeck_y_mean', y = 'ANeck_z_mean', hue='Task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9639a2-645a-49f5-b840-4776770d17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=grid_predictions, x='ABack_y_mean', y = 'ANeck_z_mean', hue='Task')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a0e9c",
   "metadata": {},
   "source": [
    "Heart Attack Analysis & Prediction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('heart.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Define explanatory variables and target variable\n",
    "X = df[['cp', 'thalachh']]\n",
    "y = df['output']\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features for SVM\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the SVM model\n",
    "model = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Support Vector Machine Classifier Results\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot decision boundary for 'cp' and 'thalachh'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='cp', y='thalachh', hue='output', data=df, palette='coolwarm')\n",
    "\n",
    "# Generate a grid to plot the decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(X['cp'].min(), X['cp'].max(), 100),\n",
    "                     np.linspace(X['thalachh'].min(), X['thalachh'].max(), 100))\n",
    "Z = model.predict(scaler.transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "plt.title(\"Decision Boundary for 'cp' and 'thalachh'\")\n",
    "plt.xlabel('cp')\n",
    "plt.ylabel('thalachh')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f79174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "df = pd.read_csv('heart.csv')\n",
    "print(\"Data Preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Define explanatory variables and target variable\n",
    "X = df[['cp', 'thalachh', 'exng', 'oldpeak']]\n",
    "y = df['output']\n",
    "\n",
    "# Pair plot for the four features against the target\n",
    "sns.pairplot(df, vars=['cp', 'thalachh', 'exng', 'oldpeak'], hue='output', palette='coolwarm')\n",
    "plt.suptitle(\"Pair Plot of Selected Features\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features for SVM\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the SVM model\n",
    "model = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Support Vector Machine Classifier Results\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eeea01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Load the Titanic dataset\n",
    "data = pd.read_csv('titanic.csv')  # Ensure the file path is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b227756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Visualize the survival rate\n",
    "sns.countplot(x='Survived', data=data)\n",
    "plt.title(\"Survival Counts\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize survival by gender\n",
    "sns.countplot(x='Survived', hue='Sex', data=data)\n",
    "plt.title(\"Survival Counts by Gender\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize survival by class\n",
    "sns.countplot(x='Survived', hue='Pclass', data=data)\n",
    "plt.title(\"Survival Counts by Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "data.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values in 'Age' and 'Embarked'\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert 'Sex' and 'Embarked' to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sex'] = label_encoder.fit_transform(data['Sex'])\n",
    "data['Embarked'] = label_encoder.fit_transform(data['Embarked'])\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop('Survived', axis=1)  # Explanatory variables\n",
    "y = data['Survived']                # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_logistic = logistic_model.predict(X_test_scaled)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logistic))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM with RBF kernel\n",
    "svm_model_rbf = SVC(kernel='rbf', class_weight='balanced')\n",
    "svm_model_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_svm_rbf = svm_model_rbf.predict(X_test_scaled)\n",
    "print(\"SVM with RBF Kernel Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm_rbf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12932d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for Logistic Regression\n",
    "logistic_cv_scores = cross_val_score(logistic_model, X_train_scaled, y_train, cv=5)\n",
    "print(\"Logistic Regression CV Accuracy:\", logistic_cv_scores.mean())\n",
    "\n",
    "# Cross-validation for SVM with RBF kernel\n",
    "svm_cv_scores = cross_val_score(svm_model_rbf, X_train_scaled, y_train, cv=5)\n",
    "print(\"SVM with RBF CV Accuracy:\", svm_cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ce76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM with adjusted class weights\n",
    "svm_model_weighted = SVC(kernel='rbf', class_weight={0: 1, 1: 2})  # Example of class weight adjustment\n",
    "svm_model_weighted.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_weighted = svm_model_weighted.predict(X_test_scaled)\n",
    "print(\"SVM with Weighted Classes Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_weighted))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fit OVR (Logistic Regression)\n",
    "ovr_model = OneVsRestClassifier(LogisticRegression(max_iter=200))\n",
    "ovr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_ovr = ovr_model.predict(X_test_scaled)\n",
    "ovr_accuracy = accuracy_score(y_test, y_pred_ovr)\n",
    "print(f\"OVR Logistic Regression Accuracy: {ovr_accuracy:.4f}\")\n",
    "\n",
    "# Fit OVO (SVM)\n",
    "ovo_model = SVC(kernel='rbf')\n",
    "ovo_model.fit(X_train_scaled, y_train)\n",
    "y_pred_ovo = ovo_model.predict(X_test_scaled)\n",
    "ovo_accuracy = accuracy_score(y_test, y_pred_ovo)\n",
    "print(f\"OVO SVM Accuracy: {ovo_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
